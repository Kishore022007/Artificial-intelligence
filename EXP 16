# Import required libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# Step 1: Load the MNIST dataset (handwritten digits)
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Step 2: Normalize the input data (0â€“255 â†’ 0â€“1)
X_train = X_train / 255.0
X_test = X_test / 255.0

# Step 3: One-hot encode the labels (e.g., 5 â†’ [0,0,0,0,0,1,0,0,0,0])
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Step 4: Build the Feedforward Neural Network
model = Sequential([
    Flatten(input_shape=(28, 28)),       # Convert 28x28 images into 784 inputs
    Dense(128, activation='relu'),       # Hidden layer 1
    Dense(64, activation='relu'),        # Hidden layer 2
    Dense(10, activation='softmax')      # Output layer (10 classes for digits 0â€“9)
])

# Step 5: Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 6: Train the model
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1)

# Step 7: Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nðŸ§® Test Accuracy: {test_acc * 100:.2f}%")

# Step 8: Make predictions (optional)
predictions = model.predict(X_test[:5])
print("\nSample Predictions (first 5 test images):")
print(predictions.argmax(axis=1))
